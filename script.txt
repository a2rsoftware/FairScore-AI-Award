Here’s a practical rundown of what tends to bite when you take LangChain to production, plus battle-tested mitigations you can drop into your stack (FastAPI/LangGraph/Ollama or cloud LLMs).


1) Orchestration drift (agents “do too much”)

Pain: Tool-using agents loop, branch unpredictably, or get stuck.
Fix:

Prefer deterministic DAGs (e.g., LangGraph) over “autonomous” agents for core flows.
Add hard stop conditions (max tool calls/turns) and explicit router nodes.
Keep tool surface small; whitelist tools and validate inputs.

2) State & memory management

Pain: Chat history/context grows, costs climb, outputs change with hidden state.
Fix:

Use checkpointers (e.g., SQLite/Redis) and make state explicit.
Trim/summary-rollup conversation history; store IDs not raw docs.
Version your state schema and prompts; write migration steps.

3) Output format instability (JSON/functions)

Pain: Models return broken JSON → downstream parsers crash.
Fix:

Use structured output (JSON schema / grammar) + validation & repair loop.
`temperature=0–0.3`; include positive/negative examples.
On failure: re-prompt with the validation error and retry (limited).

4) RAG quality & retrieval drift

Pain: Hallucinations, off-topic answers, stale indexes.
Fix:

Good chunking (400–800 tokens; 10–15% overlap), dedupe, re-index on content change.
Query rewriting and MMR; cap `k`, add reranker for long docs.
Force citations; block answers if no high-confidence sources.

5) Latency & throughput

Pain: Long prompts, large models, cold starts, slow tools.
Fix:

Cache (prompt hash → response) with TTL; pre-warm models.
Use smaller/quantized models for simple tasks; parallel map-reduce where safe.
Stream tokens to client; set timeouts (connect 2–3s, read 30–60s) + retries/backoff.

6) Cost control

Pain: Token blowups from history, retries, RAG context.
Fix:

Budget guardrails per route; truncate/compress context; cache aggressively.
Hybrid: local (Ollama 3–4B) for summarize/classify; cloud for hard reasoning.
Monitor tokens/request, cache hit rate, \$ / user / day.

7) Observability & incident response

Pain: Hard to debug runs; unclear where failures happen.
Fix:

Log inputs, outputs, tool calls, latency p50/p95/p99, retries, refusal rate.
Trace each node/step in your graph; attach run IDs to client requests.
Add /healthz and /metrics (Prometheus); write an on-call runbook.

8) Prompt injection & data exfiltration

Pain: Untrusted content can jailbreak tools or leak secrets.
Fix:

Treat RAG content as untrusted; add a gate node that filters/annotates it.
Tool allowlists, parameter validation, and sandboxes (no shell/raw net by default).
Strip secrets from prompts; store credentials out of band.

9) Versioning & reproducibility

Pain: Upgrading LangChain/model/provider changes behavior.
Fix:

Pin versions (LangChain, models, embeddings, rerankers) and prompts.
Canary deploys + evals before promoting; keep reproducible fixtures.
Tag responses with `(model, version, prompt_version)`.

10) Concurrency & async pitfalls

Pain: Mixed sync/async call stacks, blocking I/O inside event loops.
Fix:

Use the Runnable/LCEL APIs consistently; stick to async end-to-end on FastAPI.
Isolate heavy CPU (e.g., local TTS/FFmpeg) to worker processes/queues.
Bound concurrency per provider; respect rate limits.

11) Error handling & fallbacks

Pain: Transient provider errors or timeouts bubble up to users.
Fix:

Retry idempotent steps with exponential backoff; circuit breaker on repeated failure.
Define fallbacks: secondary model/provider, cached answer, or graceful refusal.

12) Data governance & compliance

Pain: PII/PHI in logs, training data, or vendor traces.
Fix:

Pre-redact PII before model calls; post-scan outputs.
Control tracing/export (turn off for sensitive flows).
Encrypt at rest; per-tenant keys; retention policies.

13) Dependency churn / breaking changes

Pain: Rapid API changes across LangChain integrations.
Fix:

Wrap providers behind a thin interface you own; minimize direct SDK usage in business logic.
Lock deps; upgrade on a schedule with tests/evals.

14) Testing & evaluation gaps

Pain: “It looks good” ≠ production quality.
Fix:

Build a golden set with expected answers & citations.
Automatic metrics: faithfulness, relevance, toxicity, format validity.
Run evals in CI for every prompt/model change; A/B test in prod.

15) Deployment environment (Windows & low-RAM specifics)

Pain: 8 GB RAM, local dev on Windows.
Fix:

Use small local models (Phi-3 mini, Qwen 3B) via Ollama; pre-pull & keep `ollama serve` warm.
Offload heavy steps (ASR, video rendering) to FFmpeg or queues; avoid giant in-memory contexts.
Keep FFmpeg and Python wheels pinned; avoid deps that don’t support Python 3.13 (e.g., `audioop` removal affected some libs).

---

#Minimal “prod-safe” wiring pattern

Graph: LangGraph DAG with explicit nodes, typed state, checkpointer (SQLite/Redis).
Contracts: JSON schema for every model output; validate, repair, or fail fast.
Resilience: Timeouts (2s connect/60s read), retries (2–3), circuit breaker, fallback.
Caching: Prompt hash cache (local file/Redis), TTL 1–24h; measure hit rate.
RAG: Chunk 400–800, MMR, k≤8, citations required; re-index schedule.
Observability: Structured logs, traces per node, metrics endpoints.
Security: Tool allowlist, input sanitization, secrets outside prompts.
Versioning: Pin model+prompt; canary + evals before release.
